{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOavwahZ_ZRq",
        "outputId": "d275a564-be5c-4d29-ccc3-c32b3fa15535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmE60Lq1CZvB",
        "outputId": "d3f82141-319b-42a1-951f-41ec8039f9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "def extract_rgb_histogram(image_path, bins_per_channel=8):\n",
        "    \"\"\"Extracts a flattened RGB histogram from an image.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:  # Check if the image was loaded successfully\n",
        "        print(f\"Warning: Unable to load image at {image_path}. Skipping...\")\n",
        "        return None  # Return None to indicate failure to load\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "\n",
        "    # Compute histograms for each channel\n",
        "    histogram = [cv2.calcHist([image], [i], None, [bins_per_channel], [0, 256]) for i in range(3)]\n",
        "    histogram = np.concatenate(histogram).flatten()\n",
        "\n",
        "    # Normalize the histogram\n",
        "    histogram = histogram / np.sum(histogram)\n",
        "    return histogram\n",
        "\n",
        "def load_data(dataset_path, bins_per_channel=8):\n",
        "    X = []\n",
        "    y = []\n",
        "    labels = os.listdir(dataset_path)\n",
        "    label_dict = {label: idx for idx, label in enumerate(labels)}\n",
        "\n",
        "    expected_histogram_length = bins_per_channel * 3  # Since you have 3 channels (RGB)\n",
        "\n",
        "    for label in labels:\n",
        "        class_path = os.path.join(dataset_path, label)\n",
        "        for image_file in os.listdir(class_path):\n",
        "            image_path = os.path.join(class_path, image_file)\n",
        "            histogram = extract_rgb_histogram(image_path, bins_per_channel)\n",
        "            if histogram is not None:\n",
        "                if len(histogram) != expected_histogram_length:\n",
        "                    print(f\"Unexpected histogram length for {image_path}. Expected: {expected_histogram_length}, Got: {len(histogram)}\")\n",
        "                    continue  # Skip this histogram\n",
        "                X.append(histogram)\n",
        "                y.append(label_dict[label])\n",
        "\n",
        "    # Ensure all elements in X have the same shape for conversion to a NumPy array\n",
        "    X = np.array(X, dtype=object)  # Use dtype=object temporarily if needed\n",
        "    # Verify if all histograms are of equal size, otherwise, investigate the cause\n",
        "    if not all(len(hist) == expected_histogram_length for hist in X):\n",
        "        raise ValueError(\"Not all histograms have the expected length. Please check the data.\")\n",
        "\n",
        "    # Correctly reshape or cast X as needed here, once you've ensured all histograms are consistent\n",
        "    # Example assuming all histograms are consistent:\n",
        "    # X = np.stack(X)  # This converts the list of arrays into a single 2D array if they are all of equal size\n",
        "\n",
        "    y = tf.keras.utils.to_categorical(y)  # Convert labels to one-hot encoding if needed\n",
        "    return X, y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_model(input_dim, num_classes):\n",
        "    \"\"\"\n",
        "    Creates a 3-layer neural network with specified input dimension and number of output classes.\n",
        "\n",
        "    Parameters:\n",
        "    - input_dim: Integer, the size of the input layer (number of features).\n",
        "    - num_classes: Integer, the number of classes for the output layer.\n",
        "\n",
        "    Returns:\n",
        "    - model: A Keras Sequential model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Dense(50, input_dim=input_dim, activation='relu'),  # First hidden layer\n",
        "        Dense(100, activation='relu'),                      # Second hidden layer\n",
        "        Dense(150, activation='relu'),                      # Third hidden layer\n",
        "        Dense(num_classes, activation='softmax')            # Output layer\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def compile_model(model):\n",
        "    \"\"\"\n",
        "    Compiles the neural network model with an optimizer, loss function, and evaluation metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - model: A Keras Sequential model.\n",
        "\n",
        "    No return value.\n",
        "    \"\"\"\n",
        "    model.compile(optimizer=Adam(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "3Qt2ANRJHkWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "dataset_path = '/content/drive/MyDrive/ColabNotebooks/SolarpanelImage/'\n",
        "X, y = load_data(dataset_path)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW243ShIb_eo",
        "outputId": "bfcf437b-278f-41cd-f56e-2830303c377b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Unable to load image at /content/drive/MyDrive/ColabNotebooks/SolarpanelImage/Clean/Cleaan (108).jpg. Skipping...\n",
            "Warning: Unable to load image at /content/drive/MyDrive/ColabNotebooks/SolarpanelImage/Clean/Cleaan (105).jpg. Skipping...\n",
            "Warning: Unable to load image at /content/drive/MyDrive/ColabNotebooks/SolarpanelImage/Clean/Cleaan (107).jpg. Skipping...\n",
            "Warning: Unable to load image at /content/drive/MyDrive/ColabNotebooks/SolarpanelImage/Clean/Cleaan (106).jpg. Skipping...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data (consider using sklearn's train_test_split)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25)\n"
      ],
      "metadata": {
        "id": "msUT_3dRHT9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 24  # This is for the 24-dimensional RGB histograms\n",
        "\n",
        "# Define the number of classes for your output layer\n",
        "num_classes = 6\n",
        "\n",
        "# Create the model\n",
        "model = create_model(input_dim=input_dim, num_classes=num_classes)\n",
        "\n",
        "# Compile the model\n",
        "compile_model(model)"
      ],
      "metadata": {
        "id": "3bfqiP-EIR8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "def train_model(model=model, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, epochs=500, batch_size=2):\n",
        "    \"\"\"\n",
        "    Trains the model on the training data with early stopping and dynamically decreasing learning rate.\n",
        "\n",
        "    Parameters:\n",
        "    - model: A Keras Sequential model.\n",
        "    - X_train: Feature data for training.\n",
        "    - y_train: Labels for training.\n",
        "    - X_val: Feature data for validation.\n",
        "    - y_val: Labels for validation.\n",
        "    - epochs: Number of epochs to train the model.\n",
        "    - batch_size: Batch size for training.\n",
        "\n",
        "    Returns:\n",
        "    - history: A history object containing training history metrics.\n",
        "    \"\"\"\n",
        "    # Initialize the EarlyStopping callback\n",
        "    early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                                   patience=10,\n",
        "                                   verbose=1,\n",
        "                                   restore_best_weights=True)\n",
        "\n",
        "    # Initialize the ReduceLROnPlateau callback\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.2,  # New learning rate = factor * previous learning rate\n",
        "                                  patience=5,  # Number of epochs with no improvement after which learning rate will be reduced\n",
        "                                  verbose=1,\n",
        "                                  min_lr=1e-6)  # Lower bound on the learning rate\n",
        "\n",
        "    # Fit the model with the training data and the callbacks\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                        callbacks=[early_stopping, reduce_lr])  # Add both callbacks here\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "pcQ7pFHKeIvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 24  # This is for the 24-dimensional RGB histograms\n",
        "\n",
        "# Define the number of classes for your output layer\n",
        "num_classes = 6\n",
        "\n",
        "# Create the model\n",
        "model = create_model(input_dim=input_dim, num_classes=num_classes)\n",
        "\n",
        "# Compile the model\n",
        "compile_model(model)"
      ],
      "metadata": {
        "id": "z-YwQ5Z6eKqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mt3ayQQIwq_",
        "outputId": "671a2f64-537a-426f-b8f2-acfe23063c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "271/271 [==============================] - 2s 3ms/step - loss: 1.6281 - accuracy: 0.3457 - val_loss: 1.4818 - val_accuracy: 0.4199 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "271/271 [==============================] - 1s 3ms/step - loss: 1.3928 - accuracy: 0.4436 - val_loss: 1.3517 - val_accuracy: 0.4475 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "271/271 [==============================] - 1s 2ms/step - loss: 1.3502 - accuracy: 0.4695 - val_loss: 1.3625 - val_accuracy: 0.4254 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "271/271 [==============================] - 1s 2ms/step - loss: 1.2860 - accuracy: 0.5046 - val_loss: 1.3033 - val_accuracy: 0.4917 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "271/271 [==============================] - 1s 2ms/step - loss: 1.2710 - accuracy: 0.5157 - val_loss: 1.3649 - val_accuracy: 0.4696 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1.2587 - accuracy: 0.5213 - val_loss: 1.3219 - val_accuracy: 0.4862 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1.2289 - accuracy: 0.5287 - val_loss: 1.3151 - val_accuracy: 0.5083 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "271/271 [==============================] - 1s 3ms/step - loss: 1.2041 - accuracy: 0.5453 - val_loss: 1.2893 - val_accuracy: 0.5028 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "271/271 [==============================] - 1s 3ms/step - loss: 1.1852 - accuracy: 0.5564 - val_loss: 1.4326 - val_accuracy: 0.4144 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "271/271 [==============================] - 1s 3ms/step - loss: 1.1703 - accuracy: 0.5453 - val_loss: 1.4153 - val_accuracy: 0.4475 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1.1618 - accuracy: 0.5730 - val_loss: 1.3387 - val_accuracy: 0.5138 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1.1437 - accuracy: 0.5749 - val_loss: 1.3119 - val_accuracy: 0.4972 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "257/271 [===========================>..] - ETA: 0s - loss: 1.1392 - accuracy: 0.5467\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.1409 - accuracy: 0.5416 - val_loss: 1.3662 - val_accuracy: 0.4586 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "271/271 [==============================] - 1s 3ms/step - loss: 1.0773 - accuracy: 0.6007 - val_loss: 1.2753 - val_accuracy: 0.5580 - lr: 2.0000e-04\n",
            "Epoch 15/500\n",
            "271/271 [==============================] - 1s 3ms/step - loss: 1.0602 - accuracy: 0.6174 - val_loss: 1.2944 - val_accuracy: 0.5028 - lr: 2.0000e-04\n",
            "Epoch 16/500\n",
            "271/271 [==============================] - 1s 3ms/step - loss: 1.0536 - accuracy: 0.6026 - val_loss: 1.2603 - val_accuracy: 0.5635 - lr: 2.0000e-04\n",
            "Epoch 17/500\n",
            "271/271 [==============================] - 1s 3ms/step - loss: 1.0512 - accuracy: 0.6044 - val_loss: 1.2733 - val_accuracy: 0.5470 - lr: 2.0000e-04\n",
            "Epoch 18/500\n",
            "271/271 [==============================] - 1s 2ms/step - loss: 1.0461 - accuracy: 0.6063 - val_loss: 1.2694 - val_accuracy: 0.5635 - lr: 2.0000e-04\n",
            "Epoch 19/500\n",
            "271/271 [==============================] - 1s 3ms/step - loss: 1.0389 - accuracy: 0.6285 - val_loss: 1.2817 - val_accuracy: 0.5470 - lr: 2.0000e-04\n",
            "Epoch 20/500\n",
            "271/271 [==============================] - 1s 2ms/step - loss: 1.0388 - accuracy: 0.6211 - val_loss: 1.2838 - val_accuracy: 0.5470 - lr: 2.0000e-04\n",
            "Epoch 21/500\n",
            "265/271 [============================>.] - ETA: 0s - loss: 1.0380 - accuracy: 0.6151\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "271/271 [==============================] - 1s 3ms/step - loss: 1.0346 - accuracy: 0.6155 - val_loss: 1.2648 - val_accuracy: 0.5635 - lr: 2.0000e-04\n",
            "Epoch 22/500\n",
            "271/271 [==============================] - 1s 3ms/step - loss: 1.0187 - accuracy: 0.6266 - val_loss: 1.2639 - val_accuracy: 0.5635 - lr: 4.0000e-05\n",
            "Epoch 23/500\n",
            "271/271 [==============================] - 1s 3ms/step - loss: 1.0160 - accuracy: 0.6303 - val_loss: 1.2651 - val_accuracy: 0.5635 - lr: 4.0000e-05\n",
            "Epoch 24/500\n",
            "271/271 [==============================] - 1s 3ms/step - loss: 1.0152 - accuracy: 0.6229 - val_loss: 1.2641 - val_accuracy: 0.5635 - lr: 4.0000e-05\n",
            "Epoch 25/500\n",
            "271/271 [==============================] - 1s 2ms/step - loss: 1.0143 - accuracy: 0.6285 - val_loss: 1.2639 - val_accuracy: 0.5691 - lr: 4.0000e-05\n",
            "Epoch 26/500\n",
            "253/271 [===========================>..] - ETA: 0s - loss: 1.0156 - accuracy: 0.6304Restoring model weights from the end of the best epoch: 16.\n",
            "\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "271/271 [==============================] - 1s 2ms/step - loss: 1.0137 - accuracy: 0.6303 - val_loss: 1.2649 - val_accuracy: 0.5691 - lr: 4.0000e-05\n",
            "Epoch 26: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(X_train).astype(np.float32)\n",
        "X_val = np.array(X_val).astype(np.float32)\n",
        "X_test = np.array(X_test).astype(np.float32)"
      ],
      "metadata": {
        "id": "wmmw_j-qJk_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train shape:\", X_train.shape, \"dtype:\", X_train.dtype)\n",
        "print(\"X_val shape:\", X_val.shape, \"dtype:\", X_val.dtype)\n",
        "print(\"X_test shape:\", X_test.shape, \"dtype:\", X_test.dtype)\n",
        "print(\"y_train shape:\", y_train.shape, \"dtype:\", y_train.dtype)\n",
        "print(\"y_val shape:\", y_val.shape, \"dtype:\", y_val.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bda9oWG2JTMH",
        "outputId": "9ca620ce-e620-47cb-a21d-c55d00b16b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (541, 24) dtype: float32\n",
            "X_val shape: (181, 24) dtype: float32\n",
            "X_test shape: (181, 24) dtype: float32\n",
            "y_train shape: (541, 6) dtype: float32\n",
            "y_val shape: (181, 6) dtype: float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCDVvZ1ALBYz",
        "outputId": "2a7f22f1-e8c0-4dc6-ef69-c60a4eaeb687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 0s 4ms/step - loss: 1.2574 - accuracy: 0.5470\n",
            "Test Loss: 1.2573987245559692\n",
            "Test Accuracy: 0.5469613075256348\n"
          ]
        }
      ]
    }
  ]
}